{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6fa6775b",
   "metadata": {},
   "source": [
    "## Ma512 final project year 2025-2026\n",
    "\n",
    "### Project Gideline \n",
    "use a pre-trained language model and to fine-tune it to fit df_train to perform a prediction of the sentiment.\n",
    "\n",
    "for this project we have decided to use **distilbert-base-uncased** as a small well suited model \n",
    "\n",
    "### Data Preprocessing and mapping\n",
    "\n",
    "This part prepares text datasets for training a sentiment classification model using DistilBERT. It first loads the training and validation datasets from CSV files. Then, it defines sentiment labels (like \"sadness\", \"joy\", etc.) and maps these labels to numeric values. The code uses the DistilBERT tokenizer to convert the text data into tokenized format, ensuring the text is padded and truncated to a fixed length. Finally, it converts the pandas DataFrames into Hugging Face Dataset objects and applies the tokenizer to both the training and validation sets, preparing them for training by setting the format for PyTorch compatibility (including input IDs and attention masks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e63dd4b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Mathis\\miniconda3\\envs\\LLM\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Map: 100%|██████████| 15999/15999 [00:09<00:00, 1705.87 examples/s]\n",
      "Map: 100%|██████████| 1999/1999 [00:01<00:00, 1555.02 examples/s]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "# Load the datasets\n",
    "df_train = pd.read_csv(\"train.csv\", sep=';')\n",
    "df_val = pd.read_csv(\"val.csv\", sep=';')\n",
    "\n",
    "# Define the labels\n",
    "labels = {\"sadness\": 0, \"joy\": 1, \"love\": 2, \"anger\": 3, \"fear\": 4, \"surprise\": 5}\n",
    "\n",
    "df_train.columns = ['text', 'label']\n",
    "df_val.columns = ['text', 'label']\n",
    "\n",
    "# Convert labels to numbers\n",
    "df_train['label'] = df_train['label'].map(labels)\n",
    "df_val['label'] = df_val['label'].map(labels)\n",
    "\n",
    "# Load the DistilBERT tokenizer\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# Tokenize the texts\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], padding=\"max_length\", truncation=True)\n",
    "\n",
    "# Create Hugging Face datasets\n",
    "train_dataset = Dataset.from_pandas(df_train[['text', 'label']])\n",
    "val_dataset = Dataset.from_pandas(df_val[['text', 'label']])\n",
    "\n",
    "# Apply the tokenizer\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "val_dataset = val_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Prepare the datasets for training\n",
    "train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "val_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde04768",
   "metadata": {},
   "source": [
    "### Model Trainning \n",
    "\n",
    "This part fine-tunes the DistilBERT model for sequence classification using the prepared datasets. It first loads the pre-trained DistilBERT model for sequence classification with the number of output labels corresponding to the sentiment labels defined earlier. Then, it sets up the training parameters using the TrainingArguments class, including options. The Trainer object is then created with the model, training arguments, and the datasets for both training and validation. Finally, the model is trained using the trainer.train() method, and after training, the fine-tuned model and tokenizer are saved to **output_dir** for later\n",
    "\n",
    "The training of the model present in this project was performed using this script, but executed in a Linux environment to achieve better performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4b005b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger le modèle DistilBERT pour la classification\n",
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=len(labels))\n",
    "\n",
    "# Définir les arguments d'entraînement\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',         # dossier de sauvegarde des résultats\n",
    "    num_train_epochs=3,             # nombre d'époques d'entraînement\n",
    "    per_device_train_batch_size=32, # taille du batch d'entraînement\n",
    "    per_device_eval_batch_size=32,  # taille du batch de validation\n",
    "    warmup_steps=100,               # nombre de pas de warmup\n",
    "    weight_decay=0.01,              # taux de decay pour l'optimiseur\n",
    "    logging_dir='./logs',           # dossier des logs\n",
    "    logging_steps=10,               # fréquence des logs\n",
    "    logging_first_step=True,        # log au premier pas d'entraînement\n",
    "    save_strategy=\"epoch\",          # sauvegarde du modèle après chaque époque\n",
    "    load_best_model_at_end=True,    # charge le meilleur modèle à la fin\n",
    "    eval_strategy=\"epoch\",          # évaluation après chaque époque\n",
    "    disable_tqdm=False,             # s'assurer que la barre de progression est activée\n",
    ")\n",
    "\n",
    "# Créer le Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,                         # modèle à entraîner\n",
    "    args=training_args,                  # paramètres d'entraînement\n",
    "    train_dataset=train_dataset,         # dataset d'entraînement\n",
    "    eval_dataset=val_dataset,            # dataset de validation\n",
    ")\n",
    "\n",
    "# Entraîner le modèle\n",
    "trainer.train()\n",
    "\n",
    "# Sauvegarder le modèle fine-tuné\n",
    "model.save_pretrained('./emotion_distilbert')\n",
    "tokenizer.save_pretrained('./emotion_distilbert')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d380aa86",
   "metadata": {},
   "source": [
    "## Model testing \n",
    "\n",
    "This part loads the fine-tuned DistilBERT model and tokenizer to perform emotion prediction on new input sentences. It first loads the model and tokenizer from the saved directory. The emotion labels are then defined to match the training labels (e.g., \"sadness\", \"joy\", etc.). A predict_emotion function is created, which tokenizes the input text, performs a prediction using the model, and returns the predicted emotion by selecting the label with the highest probability. Finally, the function is tested on a series of example sentences, predicting the emotion for each and printing the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85a40188",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: I feel like everything is going well\n",
      "Predicted emotion: joy\n",
      "\n",
      "Sentence: I feel really lonely today.\n",
      "Predicted emotion: sadness\n",
      "\n",
      "Sentence: I love you so much.\n",
      "Predicted emotion: love\n",
      "\n",
      "Sentence: I'm furious for not being listened to, it's like my opinions don't matter.\n",
      "Predicted emotion: anger\n",
      "\n",
      "Sentence: Every strange sound in the night makes me jump, I can't help but worry.\n",
      "Predicted emotion: fear\n",
      "\n",
      "Sentence: Suddenly, he did something that completely changed the game, it was astonishing.\n",
      "Predicted emotion: surprise\n",
      "\n",
      "Sentence: It's like a heavy weight is on my shoulders, and I can't bear it anymore.\n",
      "Predicted emotion: anger\n",
      "\n",
      "Sentence: I'm both excited and nervous for this new beginning, it's a whirlwind of emotions\n",
      "Predicted emotion: fear\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the fine-tuned tokenizer and model\n",
    "model_path = './emotion_distilbert'\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(model_path)\n",
    "model = DistilBertForSequenceClassification.from_pretrained(model_path)\n",
    "\n",
    "# List of possible emotions (must match the order of the labels defined during training)\n",
    "labels = [\"sadness\", \"joy\", \"love\", \"anger\", \"fear\", \"surprise\"]\n",
    "\n",
    "# Prediction function\n",
    "def predict_emotion(text):\n",
    "    # Tokenize the sentence\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
    "    \n",
    "    # Prediction\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        \n",
    "    # Find the index of the class with the highest probability\n",
    "    predicted_class_id = torch.argmax(logits, dim=-1).item()\n",
    "    \n",
    "    # Return the corresponding emotion\n",
    "    return labels[predicted_class_id]\n",
    "\n",
    "# Test on sentences of your choice\n",
    "test_sentences = [\n",
    "    \"I feel like everything is going well\", # Easy example for \"joy\"\n",
    "    \"I feel really lonely today.\",          # Easy example for \"sadness\"\n",
    "    \"I love you so much.\",                  # Easy example for \"love\"\n",
    "    \"I'm furious for not being listened to, it's like my opinions don't matter.\",       # Complex example for \"anger\"\n",
    "    \"Every strange sound in the night makes me jump, I can't help but worry.\",          # Complex example for \"fear\"\n",
    "    \"Suddenly, he did something that completely changed the game, it was astonishing.\", # Complex example for \"surprise\"\n",
    "    \"It's like a heavy weight is on my shoulders, and I can't bear it anymore.\",        # Complex example for \"sadness\"\n",
    "    \"I'm both excited and nervous for this new beginning, it's a whirlwind of emotions\" # Complex example of mixed feelings\n",
    "]\n",
    "\n",
    "# Predict the emotion for each sentence\n",
    "for sentence in test_sentences:\n",
    "    emotion = predict_emotion(sentence)\n",
    "    print(f\"Sentence: {sentence}\\nPredicted emotion: {emotion}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
